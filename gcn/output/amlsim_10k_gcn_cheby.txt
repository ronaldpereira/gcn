Calculating Chebyshev polynomials up to order 3...
Epoch: 0001 train_loss= 0.94071 train_acc= 0.29381 val_loss= 0.60093 val_acc= 1.00000 time= 94.60732
Epoch: 0002 train_loss= 0.71038 train_acc= 0.58505 val_loss= 0.45057 val_acc= 1.00000 time= 88.46271
Epoch: 0003 train_loss= 0.52259 train_acc= 0.75378 val_loss= 0.32977 val_acc= 1.00000 time= 82.69498
Epoch: 0004 train_loss= 0.37398 train_acc= 0.91445 val_loss= 0.23681 val_acc= 1.00000 time= 83.99718
Epoch: 0005 train_loss= 0.26314 train_acc= 0.99026 val_loss= 0.16679 val_acc= 1.00000 time= 81.41781
Epoch: 0006 train_loss= 0.18219 train_acc= 0.99925 val_loss= 0.11601 val_acc= 1.00000 time= 81.77182
Epoch: 0007 train_loss= 0.12592 train_acc= 0.99927 val_loss= 0.08067 val_acc= 1.00000 time= 83.21727
Epoch: 0008 train_loss= 0.08759 train_acc= 0.99926 val_loss= 0.05657 val_acc= 1.00000 time= 89.27107
Epoch: 0009 train_loss= 0.06207 train_acc= 0.99926 val_loss= 0.04030 val_acc= 1.00000 time= 76.82211
Epoch: 0010 train_loss= 0.04458 train_acc= 0.99926 val_loss= 0.02932 val_acc= 1.00000 time= 78.53220
Epoch: 0011 train_loss= 0.03339 train_acc= 0.99926 val_loss= 0.02150 val_acc= 1.00000 time= 80.91327
Epoch: 0012 train_loss= 0.02576 train_acc= 0.99926 val_loss= 0.01620 val_acc= 1.00000 time= 85.79605
Epoch: 0013 train_loss= 0.02063 train_acc= 0.99927 val_loss= 0.01243 val_acc= 1.00000 time= 78.77902
Epoch: 0014 train_loss= 0.01729 train_acc= 0.99927 val_loss= 0.00975 val_acc= 1.00000 time= 78.01846
Epoch: 0015 train_loss= 0.01500 train_acc= 0.99927 val_loss= 0.00789 val_acc= 1.00000 time= 80.18878
Epoch: 0016 train_loss= 0.01329 train_acc= 0.99927 val_loss= 0.00658 val_acc= 1.00000 time= 80.00603
Epoch: 0017 train_loss= 0.01221 train_acc= 0.99927 val_loss= 0.00562 val_acc= 1.00000 time= 82.25011
Epoch: 0018 train_loss= 0.01131 train_acc= 0.99927 val_loss= 0.00491 val_acc= 1.00000 time= 83.45332
Epoch: 0019 train_loss= 0.01068 train_acc= 0.99927 val_loss= 0.00436 val_acc= 1.00000 time= 81.12346
Epoch: 0020 train_loss= 0.01024 train_acc= 0.99927 val_loss= 0.00393 val_acc= 1.00000 time= 84.78289
Epoch: 0021 train_loss= 0.00987 train_acc= 0.99927 val_loss= 0.00358 val_acc= 1.00000 time= 84.83672
Epoch: 0022 train_loss= 0.00968 train_acc= 0.99927 val_loss= 0.00330 val_acc= 1.00000 time= 80.67038
Epoch: 0023 train_loss= 0.00955 train_acc= 0.99927 val_loss= 0.00306 val_acc= 1.00000 time= 77.48102
Epoch: 0024 train_loss= 0.00928 train_acc= 0.99928 val_loss= 0.00285 val_acc= 1.00000 time= 84.34144
Epoch: 0025 train_loss= 0.00926 train_acc= 0.99927 val_loss= 0.00267 val_acc= 1.00000 time= 79.44565
Epoch: 0026 train_loss= 0.00918 train_acc= 0.99927 val_loss= 0.00252 val_acc= 1.00000 time= 75.95630
Epoch: 0027 train_loss= 0.00901 train_acc= 0.99927 val_loss= 0.00238 val_acc= 1.00000 time= 81.60062
Epoch: 0028 train_loss= 0.00903 train_acc= 0.99928 val_loss= 0.00226 val_acc= 1.00000 time= 75.94474
Epoch: 0029 train_loss= 0.00891 train_acc= 0.99927 val_loss= 0.00215 val_acc= 1.00000 time= 80.32400
Epoch: 0030 train_loss= 0.00882 train_acc= 0.99928 val_loss= 0.00205 val_acc= 1.00000 time= 81.24285
Epoch: 0031 train_loss= 0.00879 train_acc= 0.99927 val_loss= 0.00196 val_acc= 1.00000 time= 80.70776
Epoch: 0032 train_loss= 0.00882 train_acc= 0.99927 val_loss= 0.00188 val_acc= 1.00000 time= 75.29668
Epoch: 0033 train_loss= 0.00876 train_acc= 0.99927 val_loss= 0.00181 val_acc= 1.00000 time= 79.40881
Epoch: 0034 train_loss= 0.00876 train_acc= 0.99927 val_loss= 0.00175 val_acc= 1.00000 time= 78.14674
Epoch: 0035 train_loss= 0.00869 train_acc= 0.99927 val_loss= 0.00168 val_acc= 1.00000 time= 80.65297
Epoch: 0036 train_loss= 0.00874 train_acc= 0.99927 val_loss= 0.00163 val_acc= 1.00000 time= 80.62650
Epoch: 0037 train_loss= 0.00866 train_acc= 0.99927 val_loss= 0.00158 val_acc= 1.00000 time= 81.07743
Epoch: 0038 train_loss= 0.00855 train_acc= 0.99927 val_loss= 0.00153 val_acc= 1.00000 time= 80.46056
Epoch: 0039 train_loss= 0.00864 train_acc= 0.99927 val_loss= 0.00149 val_acc= 1.00000 time= 81.89522
Epoch: 0040 train_loss= 0.00860 train_acc= 0.99927 val_loss= 0.00146 val_acc= 1.00000 time= 83.11432
Epoch: 0041 train_loss= 0.00854 train_acc= 0.99927 val_loss= 0.00142 val_acc= 1.00000 time= 78.84216
Epoch: 0042 train_loss= 0.00859 train_acc= 0.99927 val_loss= 0.00139 val_acc= 1.00000 time= 76.68506
Epoch: 0043 train_loss= 0.00854 train_acc= 0.99927 val_loss= 0.00137 val_acc= 1.00000 time= 74.76279
Epoch: 0044 train_loss= 0.00852 train_acc= 0.99927 val_loss= 0.00134 val_acc= 1.00000 time= 79.54349
Epoch: 0045 train_loss= 0.00844 train_acc= 0.99927 val_loss= 0.00133 val_acc= 1.00000 time= 80.06547
Epoch: 0046 train_loss= 0.00837 train_acc= 0.99927 val_loss= 0.00131 val_acc= 1.00000 time= 74.14025
Epoch: 0047 train_loss= 0.00842 train_acc= 0.99927 val_loss= 0.00131 val_acc= 1.00000 time= 83.54003
Epoch: 0048 train_loss= 0.00836 train_acc= 0.99927 val_loss= 0.00130 val_acc= 1.00000 time= 82.82781
Epoch: 0049 train_loss= 0.00825 train_acc= 0.99927 val_loss= 0.00131 val_acc= 1.00000 time= 84.45980
Epoch: 0050 train_loss= 0.00820 train_acc= 0.99927 val_loss= 0.00132 val_acc= 1.00000 time= 80.70345
Epoch: 0051 train_loss= 0.00811 train_acc= 0.99927 val_loss= 0.00135 val_acc= 1.00000 time= 77.40189
Early stopping...
Optimization Finished!
Test set results: cost= 0.00778 accuracy= 0.99925 time= 29.61111
F1-Score of non-Frauds: 0.888888
F1-Score of Frauds: 0.000000
F1-Score macro: 0.444444
